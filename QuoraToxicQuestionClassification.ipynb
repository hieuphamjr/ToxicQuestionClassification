{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Quora Insincere Questions Classification: 2","metadata":{"id":"P6TzjzwJ5YuM"}},{"cell_type":"markdown","source":"### Loading Libraries","metadata":{"id":"IibKbRza5SeP"}},{"cell_type":"code","source":"!pip install regex eli5 emoji","metadata":{"id":"OICt4M8u5SeQ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nimport re\nimport csv\nimport string\nimport emoji\nimport regex\nimport eli5\nimport pickle\nimport gensim\nimport spacy\nimport gc\nfrom tqdm import tqdm\nimport random\nimport sklearn\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud, STOPWORDS\nfrom scipy.sparse import hstack\nfrom IPython.display import Image\nfrom prettytable import PrettyTable\n\nfrom tqdm import tqdm_notebook\ntqdm_notebook().pandas()\n\nfrom nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer\nfrom nltk.stem.lancaster import LancasterStemmer\nfrom nltk.util import ngrams\n\nfrom sklearn.metrics import confusion_matrix, log_loss\nfrom sklearn.model_selection import train_test_split, KFold, GridSearchCV, RandomizedSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import f1_score, classification_report\nfrom sklearn.calibration import CalibratedClassifierCV","metadata":{"id":"isqWlj_p5SeR","outputId":"a73b94d8-fa41-4144-8ee3-9fcdab9bcc43","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nnltk.download('wordnet')\nnltk.download('punkt')","metadata":{"id":"SfAGU_nb5SeU","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Loading data\nĐoạn này là lload nó vào thôi nhé, k có gì cả","metadata":{"id":"-SVjgBTm5o9D"}},{"cell_type":"code","source":"df_train = pd.read_csv('../input/quora-insincere-questions-classification/train.csv')\ndf_test = pd.read_csv('../input/quora-insincere-questions-classification/train.csv')\n\nprint(\"Number of data points in training data:\", df_train.shape[0])\nprint(\"Number of data points in test data:\", df_test.shape[0])","metadata":{"id":"z-wv-iqL4-2z","outputId":"0f4cbe4c-0570-4aae-da5f-f755f43797c9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"id":"LI_tKBRK4-24","outputId":"d9689e4f-fe97-42e9-b1a5-6e189a50e4d6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Preprocessing and cleaning\nĐể ý cái pipline này, ở phía dưới là nó áp dụng lần lượt các bước này đấy","metadata":{"id":"wg9cB2qP60c_"}},{"cell_type":"markdown","source":"- Replacing math equations and url's with common abbrevation.\n- Cleaning contractions.\n- Spell Correction.\n- Removing punctuations.\n- Removing Stopwords.\n- Using WordNet Lemmatizer","metadata":{"id":"S-HsERsXQ1mU"}},{"cell_type":"code","source":"# Replacing math equations and url addresses with tags.\n# https://www.kaggle.com/canming/ensemble-mean-iii-64-36\ndef clean_tag(x):\n  if '[math]' in x:\n    x = re.sub('\\[math\\].*?math\\]', 'MATH EQUATION', x) #replacing with [MATH EQUATION]\n    \n  if 'http' in x or 'www' in x:\n    x = re.sub('(?:(?:https?|ftp):\\/\\/)?[\\w/\\-?=%.]+\\.[\\w/\\-?=%.]+', 'URL', x) #replacing with [url]\n  return x","metadata":{"id":"GH6BniD7ABhN","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# clean_punct\n# https://www.kaggle.com/canming/ensemble-mean-iii-64-36\n\npuncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', \n        '•', '~', '@', '£', '·', '_', '{', '}', '©', '^', '®', '`', '<', '→', '°', '€', '™', '›', '♥', '←', '×', '§', '″', '′', \n        '█', '…', '“', '★', '”', '–', '●', '►', '−', '¢', '¬', '░', '¡', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', \n        '—', '‹', '─', '▒', '：', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', '¯', '♦', '¤', '▲', '¸', '⋅', '‘', '∞', \n        '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '・', '╦', '╣', '╔', '╗', '▬', '❤', '≤', '‡', '√', '◄', '━', \n        '⇒', '▶', '≥', '╝', '♡', '◊', '。', '✈', '≡', '☺', '✔', '↵', '≈', '✓', '♣', '☎', '℃', '◦', '└', '‟', '～', '！', '○', \n        '◆', '№', '♠', '▌', '✿', '▸', '⁄', '□', '❖', '✦', '．', '÷', '｜', '┃', '／', '￥', '╠', '↩', '✭', '▐', '☼', '☻', '┐', \n        '├', '«', '∼', '┌', '℉', '☮', '฿', '≦', '♬', '✧', '〉', '－', '⌂', '✖', '･', '◕', '※', '‖', '◀', '‰', '\\x97', '↺', \n        '∆', '┘', '┬', '╬', '،', '⌘', '⊂', '＞', '〈', '⎙', '？', '☠', '⇐', '▫', '∗', '∈', '≠', '♀', '♔', '˚', '℗', '┗', '＊', \n        '┼', '❀', '＆', '∩', '♂', '‿', '∑', '‣', '➜', '┛', '⇓', '☯', '⊖', '☀', '┳', '；', '∇', '⇑', '✰', '◇', '♯', '☞', '´', \n        '↔', '┏', '｡', '◘', '∂', '✌', '♭', '┣', '┴', '┓', '✨', '\\xa0', '˜', '❥', '┫', '℠', '✒', '［', '∫', '\\x93', '≧', '］', \n        '\\x94', '∀', '♛', '\\x96', '∨', '◎', '↻', '⇩', '＜', '≫', '✩', '✪', '♕', '؟', '₤', '☛', '╮', '␊', '＋', '┈', '％', \n        '╋', '▽', '⇨', '┻', '⊗', '￡', '।', '▂', '✯', '▇', '＿', '➤', '✞', '＝', '▷', '△', '◙', '▅', '✝', '∧', '␉', '☭', \n        '┊', '╯', '☾', '➔', '∴', '\\x92', '▃', '↳', '＾', '׳', '➢', '╭', '➡', '＠', '⊙', '☢', '˝', '∏', '„', '∥', '❝', '☐', \n        '▆', '╱', '⋙', '๏', '☁', '⇔', '▔', '\\x91', '➚', '◡', '╰', '\\x85', '♢', '˙', '۞', '✘', '✮', '☑', '⋆', 'ⓘ', '❒', \n        '☣', '✉', '⌊', '➠', '∣', '❑', '◢', 'ⓒ', '\\x80', '〒', '∕', '▮', '⦿', '✫', '✚', '⋯', '♩', '☂', '❞', '‗', '܂', '☜', \n        '‾', '✜', '╲', '∘', '⟩', '＼', '⟨', '·', '✗', '♚', '∅', 'ⓔ', '◣', '͡', '‛', '❦', '◠', '✄', '❄', '∃', '␣', '≪', '｢', \n        '≅', '◯', '☽', '∎', '｣', '❧', '̅', 'ⓐ', '↘', '⚓', '▣', '˘', '∪', '⇢', '✍', '⊥', '＃', '⎯', '↠', '۩', '☰', '◥', \n        '⊆', '✽', '⚡', '↪', '❁', '☹', '◼', '☃', '◤', '❏', 'ⓢ', '⊱', '➝', '̣', '✡', '∠', '｀', '▴', '┤', '∝', '♏', 'ⓐ', \n        '✎', ';', '␤', '＇', '❣', '✂', '✤', 'ⓞ', '☪', '✴', '⌒', '˛', '♒', '＄', '✶', '▻', 'ⓔ', '◌', '◈', '❚', '❂', '￦', \n        '◉', '╜', '̃', '✱', '╖', '❉', 'ⓡ', '↗', 'ⓣ', '♻', '➽', '׀', '✲', '✬', '☉', '▉', '≒', '☥', '⌐', '♨', '✕', 'ⓝ', \n        '⊰', '❘', '＂', '⇧', '̵', '➪', '▁', '▏', '⊃', 'ⓛ', '‚', '♰', '́', '✏', '⏑', '̶', 'ⓢ', '⩾', '￠', '❍', '≃', '⋰', '♋', \n        '､', '̂', '❋', '✳', 'ⓤ', '╤', '▕', '⌣', '✸', '℮', '⁺', '▨', '╨', 'ⓥ', '♈', '❃', '☝', '✻', '⊇', '≻', '♘', '♞', \n        '◂', '✟', '⌠', '✠', '☚', '✥', '❊', 'ⓒ', '⌈', '❅', 'ⓡ', '♧', 'ⓞ', '▭', '❱', 'ⓣ', '∟', '☕', '♺', '∵', '⍝', 'ⓑ', \n        '✵', '✣', '٭', '♆', 'ⓘ', '∶', '⚜', '◞', '்', '✹', '➥', '↕', '̳', '∷', '✋', '➧', '∋', '̿', 'ͧ', '┅', '⥤', '⬆', '⋱', \n        '☄', '↖', '⋮', '۔', '♌', 'ⓛ', '╕', '♓', '❯', '♍', '▋', '✺', '⭐', '✾', '♊', '➣', '▿', 'ⓑ', '♉', '⏠', '◾', '▹', \n        '⩽', '↦', '╥', '⍵', '⌋', '։', '➨', '∮', '⇥', 'ⓗ', 'ⓓ', '⁻', '⎝', '⌥', '⌉', '◔', '◑', '✼', '♎', '♐', '╪', '⊚', \n        '☒', '⇤', 'ⓜ', '⎠', '◐', '⚠', '╞', '◗', '⎕', 'ⓨ', '☟', 'ⓟ', '♟', '❈', '↬', 'ⓓ', '◻', '♮', '❙', '♤', '∉', '؛', \n        '⁂', 'ⓝ', '־', '♑', '╫', '╓', '╳', '⬅', '☔', '☸', '┄', '╧', '׃', '⎢', '❆', '⋄', '⚫', '̏', '☏', '➞', '͂', '␙', \n        'ⓤ', '◟', '̊', '⚐', '✙', '↙', '̾', '℘', '✷', '⍺', '❌', '⊢', '▵', '✅', 'ⓖ', '☨', '▰', '╡', 'ⓜ', '☤', '∽', '╘', \n        '˹', '↨', '♙', '⬇', '♱', '⌡', '⠀', '╛', '❕', '┉', 'ⓟ', '̀', '♖', 'ⓚ', '┆', '⎜', '◜', '⚾', '⤴', '✇', '╟', '⎛', \n        '☩', '➲', '➟', 'ⓥ', 'ⓗ', '⏝', '◃', '╢', '↯', '✆', '˃', '⍴', '❇', '⚽', '╒', '̸', '♜', '☓', '➳', '⇄', '☬', '⚑', \n        '✐', '⌃', '◅', '▢', '❐', '∊', '☈', '॥', '⎮', '▩', 'ு', '⊹', '‵', '␔', '☊', '➸', '̌', '☿', '⇉', '⊳', '╙', 'ⓦ', \n        '⇣', '｛', '̄', '↝', '⎟', '▍', '❗', '״', '΄', '▞', '◁', '⛄', '⇝', '⎪', '♁', '⇠', '☇', '✊', 'ி', '｝', '⭕', '➘', \n        '⁀', '☙', '❛', '❓', '⟲', '⇀', '≲', 'ⓕ', '⎥', '\\u06dd', 'ͤ', '₋', '̱', '̎', '♝', '≳', '▙', '➭', '܀', 'ⓖ', '⇛', '▊', \n        '⇗', '̷', '⇱', '℅', 'ⓧ', '⚛', '̐', '̕', '⇌', '␀', '≌', 'ⓦ', '⊤', '̓', '☦', 'ⓕ', '▜', '➙', 'ⓨ', '⌨', '◮', '☷', \n        '◍', 'ⓚ', '≔', '⏩', '⍳', '℞', '┋', '˻', '▚', '≺', 'ْ', '▟', '➻', '̪', '⏪', '̉', '⎞', '┇', '⍟', '⇪', '▎', '⇦', '␝', \n        '⤷', '≖', '⟶', '♗', '̴', '♄', 'ͨ', '̈', '❜', '̡', '▛', '✁', '➩', 'ா', '˂', '↥', '⏎', '⎷', '̲', '➖', '↲', '⩵', '̗', '❢', \n        '≎', '⚔', '⇇', '̑', '⊿', '̖', '☍', '➹', '⥊', '⁁', '✢']\n\ndef clean_punct(x):\n  x = str(x)\n  for punct in puncts:\n    if punct in x:\n      x = x.replace(punct, ' ')\n    return x","metadata":{"id":"yGinDy5763K-","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Cái này là ông ta đang khai báo các từ bị phát âm sai, có thể ông này nhìn vào data để xây dựng cái list này, hoặc có thể đây là cụm từ ánh xạ sang nhau ý, tôi đoán là list này có sẵn","metadata":{}},{"cell_type":"code","source":"# correct_mispell\n# https://www.kaggle.com/oysiyl/107-place-solution-using-public-kernel\n\nmispell_dict = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'bitcoin', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization', \n                'electroneum':'bitcoin','nanodegree':'degree','hotstar':'star','dream11':'dream','ftre':'fire','tensorflow':'framework','unocoin':'bitcoin',\n                'lnmiit':'limit','unacademy':'academy','altcoin':'bitcoin','altcoins':'bitcoin','litecoin':'bitcoin','coinbase':'bitcoin','cryptocurency':'cryptocurrency',\n                'simpliv':'simple','quoras':'quora','schizoids':'psychopath','remainers':'remainder','twinflame':'soulmate','quorans':'quora','brexit':'demonetized',\n                'iiest':'institute','dceu':'comics','pessat':'exam','uceed':'college','bhakts':'devotee','boruto':'anime',\n                'cryptocoin':'bitcoin','blockchains':'blockchain','fiancee':'fiance','redmi':'smartphone','oneplus':'smartphone','qoura':'quora','deepmind':'framework','ryzen':'cpu','whattsapp':'whatsapp',\n                'undertale':'adventure','zenfone':'smartphone','cryptocurencies':'cryptocurrencies','koinex':'bitcoin','zebpay':'bitcoin','binance':'bitcoin','whtsapp':'whatsapp',\n                'reactjs':'framework','bittrex':'bitcoin','bitconnect':'bitcoin','bitfinex':'bitcoin','yourquote':'your quote','whyis':'why is','jiophone':'smartphone',\n                'dogecoin':'bitcoin','onecoin':'bitcoin','poloniex':'bitcoin','7700k':'cpu','angular2':'framework','segwit2x':'bitcoin','hashflare':'bitcoin','940mx':'gpu',\n                'openai':'framework','hashflare':'bitcoin','1050ti':'gpu','nearbuy':'near buy','freebitco':'bitcoin','antminer':'bitcoin','filecoin':'bitcoin','whatapp':'whatsapp',\n                'empowr':'empower','1080ti':'gpu','crytocurrency':'cryptocurrency','8700k':'cpu','whatsaap':'whatsapp','g4560':'cpu','payymoney':'pay money',\n                'fuckboys':'fuck boys','intenship':'internship','zcash':'bitcoin','demonatisation':'demonetization','narcicist':'narcissist','mastuburation':'masturbation',\n                'trignometric':'trigonometric','cryptocurreny':'cryptocurrency','howdid':'how did','crytocurrencies':'cryptocurrencies','phycopath':'psychopath',\n                'bytecoin':'bitcoin','possesiveness':'possessiveness','scollege':'college','humanties':'humanities','altacoin':'bitcoin','demonitised':'demonetized',\n                'brasília':'brazilia','accolite':'accolyte','econimics':'economics','varrier':'warrier','quroa':'quora','statergy':'strategy','langague':'language',\n                'splatoon':'game','7600k':'cpu','gate2018':'gate 2018','in2018':'in 2018','narcassist':'narcissist','jiocoin':'bitcoin','hnlu':'hulu','7300hq':'cpu',\n                'weatern':'western','interledger':'blockchain','deplation':'deflation', 'cryptocurrencies':'cryptocurrency', 'bitcoin':'blockchain cryptocurrency',}\n\ndef correct_mispell(x):\n  words = x.split()\n  for i in range(0, len(words)):\n    if mispell_dict.get(words[i]) is not None:\n      words[i] = mispell_dict.get(words[i])\n    elif mispell_dict.get(words[i].lower()) is not None:\n      words[i] = mispell_dict.get(words[i].lower())\n        \n  words = \" \".join(words)\n  return words","metadata":{"id":"aiEnsv5363Hn","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Cái `remove stopwords` ở dưới đây là để bỏ các từ thừa đi kiểu: \"I\" \"you\" \"a\" \"an\" \"the\" etc\nCái list ở bên kia kìa :3","metadata":{}},{"cell_type":"code","source":"# remove stopwords\ndef remove_stopwords(x):\n  x = [word for word in x.split() if word not in STOPWORDS]\n  x = ' '.join(x)\n\n  return x","metadata":{"id":"gG0IuCDL8CXZ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Bỏ mấy từ viết tắt này","metadata":{}},{"cell_type":"code","source":"# clean word contractions\n## https://www.kaggle.com/theoviel/improve-your-score-with-text-preprocessing-v2 \n\ncontraction_mapping = {\"We'd\": \"We had\", \"That'd\": \"That had\", \"AREN'T\": \"Are not\", \"HADN'T\": \"Had not\", \"Could've\": \"Could have\", \"LeT's\": \"Let us\", \"How'll\": \"How will\", \"They'll\": \"They will\", \"DOESN'T\": \"Does not\", \"HE'S\": \"He has\", \"O'Clock\": \"Of the clock\", \"Who'll\": \"Who will\", \"What'S\": \"What is\", \"Ain't\": \"Am not\", \"WEREN'T\": \"Were not\", \"Y'all\": \"You all\", \"Y'ALL\": \"You all\", \"Here's\": \"Here is\", \"It'd\": \"It had\", \"Should've\": \"Should have\", \"I'M\": \"I am\", \"ISN'T\": \"Is not\", \"Would've\": \"Would have\", \"He'll\": \"He will\", \"DON'T\": \"Do not\", \"She'd\": \"She had\", \"WOULDN'T\": \"Would not\", \"She'll\": \"She will\", \"IT's\": \"It is\", \"There'd\": \"There had\", \"It'll\": \"It will\", \"You'll\": \"You will\", \"He'd\": \"He had\", \"What'll\": \"What will\", \"Ma'am\": \"Madam\", \"CAN'T\": \"Can not\", \"THAT'S\": \"That is\", \"You've\": \"You have\", \"She's\": \"She is\", \"Weren't\": \"Were not\", \"They've\": \"They have\", \"Couldn't\": \"Could not\", \"When's\": \"When is\", \"Haven't\": \"Have not\", \"We'll\": \"We will\", \"That's\": \"That is\", \"We're\": \"We are\", \"They're\": \"They' are\", \"You'd\": \"You would\", \"How'd\": \"How did\", \"What're\": \"What are\", \"Hasn't\": \"Has not\", \"Wasn't\": \"Was not\", \"Won't\": \"Will not\", \"There's\": \"There is\", \"Didn't\": \"Did not\", \"Doesn't\": \"Does not\", \"You're\": \"You are\", \"He's\": \"He is\", \"SO's\": \"So is\", \"We've\": \"We have\", \"Who's\": \"Who is\", \"Wouldn't\": \"Would not\", \"Why's\": \"Why is\", \"WHO's\": \"Who is\", \"Let's\": \"Let us\", \"How's\": \"How is\", \"Can't\": \"Can not\", \"Where's\": \"Where is\", \"They'd\": \"They had\", \"Don't\": \"Do not\", \"Shouldn't\":\"Should not\", \"Aren't\":\"Are not\", \"ain't\": \"is not\", \"What's\": \"What is\", \"It's\": \"It is\", \"Isn't\":\"Is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }\n\ndef clean_contractions(text):\n    specials = [\"’\", \"‘\", \"´\", \"`\"]\n    for s in specials:\n        text = text.replace(s, \"'\")\n    \n    text = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in text.split(\" \")])\n    return text","metadata":{"id":"yUvNn9jW8sMX","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Cái lematizing chuyển từ về dạng gốc của nó, kiểu teacher, teaches thành teach; best, better thành good; đại loại vậy","metadata":{}},{"cell_type":"code","source":"# word lemmatizing\n\nlemmatizer = WordNetLemmatizer()\ndef lemma_text(x):\n  x = x.split()\n  x = [lemmatizer.lemmatize(word) for word in x]\n  x = ' '.join(x)\n\n  return x","metadata":{"id":"NtFry3bK88Z_","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Áp dụng tất cả các cái bên trên vào 1 hàm :3","metadata":{}},{"cell_type":"code","source":"def data_cleaning(x):\n  x = clean_tag(x)\n  x = clean_punct(x)\n  x = correct_mispell(x)\n  x = remove_stopwords(x)\n  x = clean_contractions(x)\n  x = lemma_text(x)\n  return x","metadata":{"id":"gXBCN3Oo9dcq","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Clean toàn bộ data (tập train với test)","metadata":{}},{"cell_type":"code","source":"# preprocessing given train and test data\ndf_train['preprocessed_question_text'] = df_train['question_text'].progress_map(lambda x: data_cleaning(x))\ndf_test['preprocessed_question_text'] = df_test['question_text'].progress_map(lambda x: data_cleaning(x))","metadata":{"id":"k8mloM359dSt","outputId":"32059bda-f63a-4ffc-838c-aa3fdca31695","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"Ở trên này mới là preprocessing này, bên dưới là thống kê hoy","metadata":{}},{"cell_type":"markdown","source":"### Basic Feature Extraction","metadata":{"id":"-o2KqO-cG2YD"}},{"cell_type":"markdown","source":"Chia các câu thành các từ, ngta gọi đây là tokenization, token hóa ý. Câu như \"Basic Feature Extraction\" token ra thì là ['Basic', 'Feature', 'Extraction'] đại loại vậy\n\nCả trên tập train và test","metadata":{}},{"cell_type":"code","source":"# Number of words\ndf_train['num_words'] = df_train['question_text'].apply(lambda x: len(str(x).split()))\ndf_test['num_words'] = df_test['question_text'].apply(lambda x: len(str(x).split()))\n\n# Number of capital_letters\ndf_train['num_capital_let'] = df_train['question_text'].apply(lambda x: len([c for c in str(x) if c.isupper()]))\ndf_test['num_capital_let'] = df_test['question_text'].apply(lambda x: len([c for c in str(x) if c.isupper()]))\n\n# Number of unique words\ndf_train['num_unique_words'] = df_train['question_text'].apply(lambda x: len(set(str(x).split())))\ndf_test['num_unique_words'] = df_test['question_text'].apply(lambda x: len(set(str(x).split())))\n\n# Number of characters\ndf_train['num_char'] = df_train['question_text'].apply(lambda x: len(str(x)))\ndf_test['num_char'] = df_test['question_text'].apply(lambda x: len(str(x)))\n\n# Number of stopwords\ndf_train['num_stopwords'] = df_train['question_text'].apply(lambda x: len([c for c in str(x).lower().split() if c in STOPWORDS]))\ndf_test['num_stopwords'] = df_test['question_text'].apply(lambda x: len([c for c in str(x).lower().split() if c in STOPWORDS]))\n\ndf_train.head()","metadata":{"id":"-NF51YEpHI41","outputId":"c4e6c4fc-609c-44a9-cf39-d787a40d04c9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\"HEY YOUUUU!!!!!\", \"hey you.\",","metadata":{}},{"cell_type":"markdown","source":"Does does DOES","metadata":{}},{"cell_type":"markdown","source":"Đây là làm chút thống kê này, num_words là từ trong câu, num_capital chắc là số từ viết hoa, unique_words thì là các từ k lặp, quan tâm ở cái num_stopwords thôi, vì sau mình phải bỏ stopwords, à không mình bỏ ở cái bên trên luôn r.\n\nNhưng chú để ý chỗ cái preprocessed k, khi mà preprocessed xong thì đúng lý ra sẽ chữ sẽ phải là chữ thường/hoa hết (1 trong 2 loại), ác dấu là bị bỏ r kiểu â@#$%^&*()<>?:{}_+ là phải bị bỏ r","metadata":{}},{"cell_type":"markdown","source":"### Train, Test & Val split\n","metadata":{"id":"59xa5YkyP9JP"}},{"cell_type":"markdown","source":"Chia data Train ra thành 2 phần: Train và Val (Tập Test không có target nên phải cần 1 tập khác để kiểm tra máy đang học đúng hay sai)","metadata":{}},{"cell_type":"code","source":"y = df_train['target'].values\nX_train, X_test, y_train, y_test = train_test_split(df_train, y, test_size=0.2, random_state=2019)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=2019)\n\nprint('X_train: ', X_train.shape, y_train.shape)\nprint('X_test: ',X_test.shape, y_test.shape)\nprint('X_val: ',X_val.shape, y_val.shape)","metadata":{"id":"xswiSfSKP9JT","outputId":"080d0c2b-7391-41ac-fe48-a23cdd850c5c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check for percentage of class disb in train test split.\n\ndef plot_class_disb(class_disb, data, disb_name):\n  class_disb.plot(kind=\"bar\")\n  plt.xlabel('class')\n  plt.ylabel('Datapoints per class')\n  plt.title(f'Distribution of yi in {disb_name}')\n  plt.grid(True)\n  \n  sorted_yi = np.argsort(-class_disb.values)\n  print(disb_name,':')\n  for i in sorted_yi:\n    print('Number of data points in class', i, ':', class_disb.values[i], '(', np.round((class_disb.values[i]/data.shape[0]*100), 3), '%)')\n  \n  print('-'*50)","metadata":{"id":"WIkZQaViP9Jb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Kiểm tra xem các class ở trong các file có cân bằng không. Tránh các trường hợp kiểu chia xong file train vs val mà train chỉ gồm mỗi 1 class hay val chỉ có mỗi 1 class","metadata":{}},{"cell_type":"code","source":"# distribution of yi in train, test and val\n\nplt.subplot(1, 3, 1)\nplot_class_disb(\n  X_train['target'].value_counts().sort_values(),\n  X_train,\n  'TRAIN')\n         \nplt.subplot(1, 3, 2)\nplot_class_disb(\n  X_test['target'].value_counts().sort_values(),\n  X_test,\n  'TEST')\n\nplt.subplot(1, 3, 3)\nplot_class_disb(\n  X_val['target'].value_counts().sort_values(),\n  X_val,\n  'VAILIDATION')\nplt.subplots_adjust(right=2.0)\nplt.subplots_adjust(top=1)\nplt.show()","metadata":{"id":"R2lLb56eP9Jd","outputId":"fe951eb2-2a45-4e0a-cc77-0b48587314c3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Baseline Model(LR) with basic extracted features","metadata":{"id":"RAO0b45ZQWZS"}},{"cell_type":"code","source":"tfidf = TfidfVectorizer(stop_words='english', ngram_range=(1, 3))\ntfidf.fit_transform(list(df_train['preprocessed_question_text'].values) + list(df_test['preprocessed_question_text'].values))\n\nX_train_ques = tfidf.transform(X_train['preprocessed_question_text'].values)\nX_test_ques = tfidf.transform(X_test['preprocessed_question_text'].values)\nX_val_ques = tfidf.transform(X_val['preprocessed_question_text'].values)\n\nprint(X_train_ques.shape)\nprint(X_test_ques.shape)\nprint(X_val_ques.shape)","metadata":{"id":"nNb8-zlPQWZd","outputId":"54cae894-0f46-44e8-a30f-72dc7e1dd8e8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Standardize stats features\nfrom sklearn.preprocessing import StandardScaler\n\n# number of words\nnum_words =  StandardScaler()\nX_train_num_words = num_words.fit_transform(X_train['num_words'].values.reshape(-1, 1))\nX_test_num_words = num_words.transform(X_test['num_words'].values.reshape(-1, 1))\nX_val_num_words = num_words.transform(X_val['num_words'].values.reshape(-1, 1))\n\n# number of unique words\nnum_unique_words =  StandardScaler()\nX_train_num_unique_words = num_unique_words.fit_transform(X_train['num_unique_words'].values.reshape(-1, 1))\nX_test_num_unique_words = num_unique_words.transform(X_test['num_unique_words'].values.reshape(-1, 1))\nX_val_num_unique_words = num_unique_words.transform(X_val['num_unique_words'].values.reshape(-1, 1))\n\n# number of char\nnum_char =  StandardScaler()\nX_train_num_char = num_char.fit_transform(X_train['num_char'].values.reshape(-1, 1))\nX_test_num_char = num_char.transform(X_test['num_char'].values.reshape(-1, 1))\nX_val_num_char = num_char.transform(X_val['num_char'].values.reshape(-1, 1))\n\n# number of stopwords\nnum_stopwords =  StandardScaler()\nX_train_num_stopwords = num_stopwords.fit_transform(X_train['num_stopwords'].values.reshape(-1, 1))\nX_test_num_stopwords = num_stopwords.transform(X_test['num_stopwords'].values.reshape(-1, 1))\nX_val_num_stopwords = num_stopwords.transform(X_val['num_stopwords'].values.reshape(-1, 1))","metadata":{"id":"jti8vx_fQWZo","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lợi dụng các đặc trưng về \"số lượng từ (num_word)\", \"số lượng từ unique\", ... và stack vào để cho model thống kê!","metadata":{}},{"cell_type":"code","source":"# Stacking features \n\nX_tr = hstack((\n    X_train_ques,\n    X_train_num_words,\n    X_train_num_unique_words,\n    X_train_num_char,\n    X_train_num_stopwords\n))\n\nX_te = hstack((\n    X_test_ques,\n    X_test_num_words,\n    X_test_num_unique_words,\n    X_test_num_char,\n    X_test_num_stopwords\n))\n\nX_cv = hstack((\n    X_val_ques,\n    X_val_num_words,\n    X_val_num_unique_words,\n    X_val_num_char,\n    X_val_num_stopwords\n))\n\nprint(X_tr.shape, y_train.shape)\nprint(X_te.shape, y_test.shape)\nprint(X_cv.shape, y_val.shape)","metadata":{"id":"ote82PP0QWZq","outputId":"956c0522-0bc3-4c74-fb6b-08d1e9edb213","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"parmas = {'C': [0.001, 0.001, 0.1, 1, 10]}\n\ngridsearch = GridSearchCV(LogisticRegression(), parmas, scoring='f1', n_jobs=-1, verbose=1)\ngridsearch.fit(X_tr, y_train)","metadata":{"id":"KKKXGsGoQWZv","outputId":"c80b6ca3-d823-4145-e4e0-51b690981ebb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gridsearch.best_params_","metadata":{"id":"AfoSIoeYY87g","outputId":"5b7c4462-b972-4f15-d0b1-7a8bffa6c274","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf = LogisticRegression(C=1)\nclf.fit(X_tr, y_train)","metadata":{"id":"YdlZgQeiQWZ2","outputId":"31008baf-0b3d-4525-e849-d65e184ff3e7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_val = clf.predict_proba(X_cv)[:,1]\n\nfor t in np.arange(0.1, 0.201, 0.01):\n  t = np.round(t, 2)\n  print('F1 score at threshold {0} is {1}'.format(t, f1_score(y_val, (y_pred_val>t).astype('int'))))","metadata":{"id":"SuGD2Q1NQWZ6","outputId":"146ba8ec-3f17-4944-e37b-5713ca238c1e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t = 0.17\ny_pred_test = clf.predict_proba(X_te)[:,1]\nprint('F1-Score: ', round(f1_score(y_test, (y_pred_test>t)), 5))","metadata":{"id":"dkc9_TyFQWZ9","outputId":"a03fcca7-c7bf-4b95-e57e-a337c45f4230","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_confusion_matrix(y_test, y_pred):\n  cm = confusion_matrix(y_test, y_pred)\n  plt.figure(figsize=(5, 3))\n  sns.heatmap(cm, annot=True, fmt='d')\n  plt.title('Confusion Matrix')\n  plt.show()\n  \n  print(f\"Correctly classified sincere questions: {round(cm[0][0]/(cm[0][0] + cm[0][1])*100, 2)}%\")\n  print(f\"Correctly classified insincere questions: {round(cm[1][1]/(cm[1][0] + cm[1][1])*100, 2)}%\")","metadata":{"id":"FCjF4X3lQWaG","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(y_test, (y_pred_test>t))","metadata":{"id":"oKfvuda2QWaJ","outputId":"59a28ec6-bd93-44d4-b4f9-f705c6e5edb4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, (y_pred_test>t)))","metadata":{"id":"T_0lFR2fQWaN","outputId":"f40d1f46-cee4-4672-ec67-3f2fe0a38a4c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eli5.show_weights(clf, vec=tfidf, top=50, feature_filter=lambda x:x != '<BIAS>')","metadata":{"id":"mQSRdpZNQWaR","outputId":"835b2b6a-1c5b-4830-e92f-894d00353b26","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display_incorrect_predictions(data, pred):\n  \"\"\"\n  Displays text for both correct|incorrect pred for classes(0, 1)\n  \"\"\"\n  \n  df_classified = data.copy()\n  df_classified['pred'] = (pred>t)\n  \n  print('\\033[1m'+'Sample Class Prediction Anaylsis')\n  print('')\n  \n  print('\\033[1m'+'Sincere question:\\033[34m Correct prediction')\n  print('\\033[0m')\n  print(data[(df_classified['pred'] == False) & (df_classified['target'] ==0) & (df_classified['num_words'] <15)]['question_text'].head(5).values)\n  print('\\033[1m'+'Sincere question:\\033[31m Incorrect prediction')\n  print('\\033[0m')\n  print(data[(df_classified['pred'] == True) & (df_classified['target'] ==0) & (df_classified['num_words'] <15)]['question_text'].head(5).values)\n  \n  print('-'*50)\n  \n  print('\\033[1m'+'Insincere question :\\033[34m Correct prediction')\n  print('\\033[0m')\n  print(data[(df_classified['pred'] == True) & (df_classified['target'] ==1) & (df_classified['num_words'] <15)]['question_text'].head(5).values)\n  print('\\033[1m'+'Insincere question:\\033[31m Incorrect Prediction')\n  print('\\033[0m')\n  print(data[(df_classified['pred'] == False) & (df_classified['target'] ==1) & (df_classified['num_words'] <15)]['question_text'].head(5).values)\n  ","metadata":{"id":"wyD4I-sBQWaW","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display_incorrect_predictions(X_test, (y_pred_test>t))","metadata":{"id":"HGub20zFQWaX","outputId":"a4d201e8-48b8-4224-bbb8-1ada0aba2633","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction","metadata":{}},{"cell_type":"code","source":"y_pred_res = clf.predict(X_te)\nprint(len(y_pred_res))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp = X_test['qid'].to_dict()\nqid = []\nfor (key, val) in temp.items():\n    qid.append(val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(qid[:4])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\n\npb = tqdm(range(len(y_pred_res)))\nres = {}\n\nfor i in pb:\n    res[qid[i]] = y_pred_res[i]\n\n# print(res)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import csv\nwith open('submission.csv','w', newline='') as csv_file:\n    fieldnames = ['qid', 'prediction']\n    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n\n    writer.writeheader()\n    for (key, val) in res.items():\n        writer.writerow({'qid': key, 'prediction': val})\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}